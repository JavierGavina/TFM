{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src import dataloader\n",
    "from src.constants import constants\n",
    "from src.dataloader import labelEncoding, labelDecoding\n",
    "from models.cGAN_300_300 import cGAN\n",
    "from models.callbacks import LoggingCheckpointTraining, SaveImageTraining\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Conv2DTranspose, Reshape, BatchNormalization, LeakyReLU, Dropout, Input, Concatenate, Embedding\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import History, ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "get_data = dataloader.DataLoader(\n",
    "    data_dir=f\"{constants.data.FINAL_PATH}/groundtruth.csv\",\n",
    "    aps_list=constants.aps, batch_size=30, step_size=5,\n",
    "    size_reference_point_map=28, return_axis_coords=False\n",
    ")\n",
    "X, y, _ = get_data()\n",
    "minimo, maximo = np.min(X), np.max(X)\n",
    "X_reescalado = 2 * (X - minimo) / (maximo - minimo) - 1\n",
    "y_encoded = labelEncoding(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/25 [=========================>....] - ETA: 0s - loss_d: 0.4160 - loss_g: 0.6874"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (28,28) into shape (300,300)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 96\u001B[0m\n\u001B[0;32m     88\u001B[0m cgan\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[0;32m     89\u001B[0m     Adam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m, beta_1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m),\n\u001B[0;32m     90\u001B[0m     Adam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m, beta_1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m),\n\u001B[0;32m     91\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mBinaryCrossentropy(),\n\u001B[0;32m     92\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mBinaryCrossentropy()\n\u001B[0;32m     93\u001B[0m )\n\u001B[0;32m     95\u001B[0m \u001B[38;5;66;03m# train model\u001B[39;00m\n\u001B[1;32m---> 96\u001B[0m \u001B[43mcgan\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;66;03m# Curvas de aprendizaje\u001B[39;00m\n\u001B[0;32m     99\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(hist\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss_d\u001B[39m\u001B[38;5;124m\"\u001B[39m], label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss_d\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\Desktop\\SISTEMAS INTELIGENTES\\cGAN_DataAumentation_GitHub\\TFM\\models\\callbacks.py:66\u001B[0m, in \u001B[0;36mSaveImageTraining.on_epoch_end\u001B[1;34m(self, epoch, logs)\u001B[0m\n\u001B[0;32m     64\u001B[0m random_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m/\u001B[39m n)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n):\n\u001B[1;32m---> 66\u001B[0m     real_img[label] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX[np\u001B[38;5;241m.\u001B[39mwhere(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my \u001B[38;5;241m==\u001B[39m label)[\u001B[38;5;241m0\u001B[39m], :, :, \u001B[38;5;241m0\u001B[39m][random_idx]\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# Plot generated vs real images\u001B[39;00m\n\u001B[0;32m     69\u001B[0m fig, axs \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m3\u001B[39m, n, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m10\u001B[39m), dpi\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: could not broadcast input array from shape (28,28) into shape (300,300)"
     ]
    }
   ],
   "source": [
    "def define_discriminator(input_shape=(28,28,1), n_classes=7):\n",
    "    input_label = Input(shape=(1,), name=\"input_label\")\n",
    "    lab = Embedding(n_classes, 50)(input_label)\n",
    "    lab = Dense(input_shape[0]*input_shape[1])(lab)\n",
    "    lab = Reshape((input_shape[0], input_shape[1], 1))(lab)\n",
    "\n",
    "    input_image = Input(shape=input_shape, name=\"input_image\")\n",
    "    combined = Concatenate(name=\"concatenate\")([input_image, lab])\n",
    "\n",
    "    x = Conv2D(64, (3,3), strides=(2,2), padding=\"same\")(combined)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, (3,3), strides=(2,2), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(256, (3,3), strides=(2,2), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(1, activation=\"sigmoid\", name=\"out_layer\")(x)\n",
    "\n",
    "    model = Model([input_image, input_label], out, name=\"discriminator\")\n",
    "    return model\n",
    "\n",
    "def define_generator(latent_dim=100, n_classes=7):\n",
    "    input_label = Input(shape=(1,), name=\"input_label\")\n",
    "    lab = Embedding(n_classes, 50)(input_label)\n",
    "    lab = Dense(7*7*128)(lab)\n",
    "    lab = Reshape((7, 7, 128))(lab)\n",
    "\n",
    "    input_latent = Input(shape=(latent_dim,), name=\"input_noise\")\n",
    "    x = Dense(7*7*128)(input_latent)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Reshape((7, 7, 128))(x)\n",
    "\n",
    "    combined = Concatenate(name=\"concatenate\")([x, lab])\n",
    "\n",
    "    x = Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"same\")(combined)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2DTranspose(64, (4,4), strides=(2,2), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    out = Conv2D(1, (1,1), activation=\"tanh\", padding=\"same\")(x)\n",
    "\n",
    "    model = Model([input_latent, input_label], out, name=\"generator\")\n",
    "    return model\n",
    "\n",
    "discriminator = define_discriminator()\n",
    "generator = define_generator(latent_dim=100)\n",
    "\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, show_layer_activations=True)\n",
    "\n",
    "path_cgan_28 = \"outputs/process_training/cGAN_28_28\"\n",
    "path_cgan_28_checkpoints = f\"{path_cgan_28}/checkpoints\"\n",
    "path_cgan_28_images = f\"{path_cgan_28}/images\"\n",
    "path_cgan_28_learning_curves = f\"{path_cgan_28}/learning_curves\"\n",
    "\n",
    "# define the training dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_reescalado, y_encoded)).shuffle(1000).batch(64)\n",
    "\n",
    "# define callbacks\n",
    "save_image = SaveImageTraining(X_reescalado, y_encoded, save_dir=path_cgan_28_images)\n",
    "save_model = LoggingCheckpointTraining(save_dir=path_cgan_28_checkpoints)\n",
    "hist = History()\n",
    "decay_lr = ReduceLROnPlateau(monitor='loss_g', factor=0.5, patience=5, verbose=1, min_lr=0.00001)\n",
    "\n",
    "callbacks = [\n",
    "    save_image,\n",
    "    save_model,\n",
    "    hist,\n",
    "    decay_lr\n",
    "]\n",
    "\n",
    "# modelo cGAN\n",
    "cgan = cGAN(generator, discriminator)\n",
    "\n",
    "# compile model\n",
    "cgan.compile(\n",
    "    Adam(learning_rate=0.001, beta_1=0.5),\n",
    "    Adam(learning_rate=0.001, beta_1=0.5),\n",
    "    tf.keras.losses.BinaryCrossentropy(),\n",
    "    tf.keras.losses.BinaryCrossentropy()\n",
    ")\n",
    "\n",
    "# train model\n",
    "cgan.fit(dataset, epochs=50, callbacks=callbacks)\n",
    "\n",
    "# Curvas de aprendizaje\n",
    "plt.plot(hist.history[\"loss_d\"], label=\"loss_d\")\n",
    "plt.plot(hist.history[\"loss_g\"], label=\"loss_g\")\n",
    "plt.title(\"Curvas de aprendizaje\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{path_cgan_28_learning_curves}/learning_curves.png\")\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
